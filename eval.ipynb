{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Literal, List\n",
    "\n",
    "class QuestionSetSpecs(BaseModel):\n",
    "\tset: Literal[\"hotpotqa\", \"triviaqa\"]\n",
    "\tsubset: Optional[Literal[\"easy\", \"medium\", \"hard\"]] = None\n",
    "\tsamples: int\n",
    "\tbatch_size: int\n",
    "\tseed: int\n",
    "\tmodel: str\n",
    "\tgt_answer_model: str # Needed for triviaqa, which has no GT answers.\n",
    "\tevaluator_model: str # For grading the model's answers\n",
    "\t\n",
    "\tql_api_key: str\n",
    "\topenai_api_key: str\n",
    "\tdeep_infra_api_key: str\n",
    " \n",
    "\t\n",
    "\t\n",
    "\t# Providing and int `n` here means we will test cases\n",
    "\t# 1, 2, 3, ..., n, plus the unbounded case.\n",
    "\t# Providing `None` means we will only test the unbounded case.\n",
    "\tmax_search_depth: Optional[int | None] = None\n",
    "\trerank_n: Optional[int] = None\n",
    "\tinclude_chunk_metadata: Optional[bool] = False\n",
    "\tunbounded_cases: Optional[List[str] | None] = None\n",
    " \n",
    "\tholdout: Optional[str | None] = None\n",
    " \n",
    "\n",
    "\n",
    "DATASET_SPECS = QuestionSetSpecs(\n",
    "\t# set=\"hotpotqa\",\n",
    "\tset=\"triviaqa\",\n",
    "\t# subset=\"hard\",\n",
    "\tsamples=800,\n",
    "\tbatch_size=3,\n",
    "\tseed=43,\n",
    "\t# model=\"deepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "\t# model=\"openai/gpt-4o\",\n",
    "\t# model=\"openai/gpt-4o-mini\",\n",
    "\t# model=\"deepinfra/meta-llama/Llama-3.3-70B-Instruct\",\n",
    "\tmodel=\"llama-3.1-8b-instruct-sgs-tuned\",\n",
    "\tgt_answer_model=\"openai/gpt-4o-mini\",\n",
    "\tevaluator_model=\"openai/gpt-4o-mini\",\n",
    "\tmax_search_depth=5,\n",
    "\trerank_n=100,\n",
    "\tinclude_chunk_metadata=True,\n",
    "\tunbounded_cases=[\"BM25\", \"HS\", \"HS_RR\"],\n",
    "\t\n",
    "\t# unbounded_cases=[\"BM25\"],\n",
    "\t\n",
    "\tholdout=\"finetune_train_set_question_hashes.json\",\n",
    "\t\n",
    "\t# Local QL Deployment API key.\n",
    "\tql_api_key=\"sk-peX2gbjszf2HdTJ5aqqcmb4iwhLJvidG9YNYILBedz0gXVJT\",\n",
    "\topenai_api_key=\"\",\t\t# Load your own key here\n",
    "\tdeep_infra_api_key=\"\"\t# Load your own key here\n",
    ")\n",
    "\n",
    "\n",
    "# QL Document Collection IDs for RAG\n",
    "TRIVIAQA_ALL_ARTICLES = \"J0N8YoPAykCmSG5Q5iHcWKCMvR9I6QUN\"\n",
    "HOTPOTQA_ALL_ARTICLES = \"OKT6X298qw2H2chedi4Y12Nnhk4u1RXd\"\n",
    "\n",
    "TARGET_COLLECTIONS = [HOTPOTQA_ALL_ARTICLES] if DATASET_SPECS.set == \"hotpotqa\" else [TRIVIAQA_ALL_ARTICLES]\n",
    "\n",
    "QUESTION_SELECTIONS = [] # Each entry must be dict with `question`, `answer`, and `pages` keys\n",
    "\n",
    "def hash_string(input_string: str) -> str:\n",
    "    # Create a new sha256 hash object\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    \n",
    "    # Update the hash object with the bytes of the input string\n",
    "    sha256_hash.update(input_string.encode('utf-8'))\n",
    "    \n",
    "    # Get the hexadecimal representation of the hash\n",
    "    hashed_string = sha256_hash.hexdigest()\n",
    "    \n",
    "    return hashed_string\n",
    "\n",
    "# Jupyter cd to current script dir\n",
    "try:\n",
    "    os.chdir(globals()['_dh'][0])\n",
    "except:\n",
    "\tos.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "\n",
    "CORRECT_ANSWERS_LOOKUP_HOTPOT = {}\n",
    "EVAL_RESULTS = {}\n",
    "\n",
    "OUTPUT_FILE = f\"outputs_{'hotpot' if DATASET_SPECS.set == 'hotpotqa' else 'trivia'}.json\"\n",
    "\n",
    "\n",
    "if not os.path.exists(\"correct_answers_lookup.json\"):\n",
    "\twith open(\"correct_answers_lookup.json\", \"w\") as f:\n",
    "\t\tjson.dump({}, f, indent=4)\n",
    "\t\tf.close()\n",
    "  \n",
    "if not os.path.exists(\"correct_answers_lookup_hotpot.json\"):\n",
    "\twith open(\"correct_answers_lookup_hotpot.json\", \"w\") as f:\n",
    "\t\tjson.dump({}, f, indent=4)\n",
    "\t\tf.close()\n",
    "\n",
    "if not os.path.exists(OUTPUT_FILE):\n",
    "\twith open(OUTPUT_FILE, \"w\") as f:\n",
    "\t\tjson.dump({}, f, indent=4)\n",
    "\t\tf.close()\n",
    "\n",
    "with open(\"correct_answers_lookup_hotpot.json\", \"r\") as f:\n",
    "\tCORRECT_ANSWERS_LOOKUP_HOTPOT = json.load(f)\n",
    "\tf.close()\n",
    "\n",
    "with open(\"correct_answers_lookup.json\", \"r\") as f:\n",
    "\tCORRECT_ANSWERS_LOOKUP_TRIVIA = json.load(f)\n",
    "\tf.close()\n",
    "\n",
    "with open(OUTPUT_FILE, \"r\") as f:\n",
    "\tEVAL_RESULTS = json.load(f)\n",
    "\tf.close()\n",
    "\n",
    "# TEST_MODEL_RESPONSE_LOOKUP[\"gpt-4o-mini\"] = {}\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "parent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "dir_over = os.path.join(parent_dir, 'hotpotqa')\n",
    "\n",
    "print(current_dir)\n",
    "print(parent_dir)\n",
    "print(dir_over)\n",
    "\n",
    "MAX_EVALUATIONS = DATASET_SPECS.samples\n",
    "\n",
    "\n",
    "# with open(os.path.join(dir_over, 'hotpot_train_v1.1.json'), 'r') as f:\n",
    "# \tHOTPOT_ALL = json.load(f)\n",
    "# \tf.close()\n",
    "\n",
    "# def hotpot_context_to_document(context):\n",
    "# \ttitle = context[0]\n",
    "# \tparagraphs = context[1]\n",
    "# \tp_joined = '\\n'.join(paragraphs)\n",
    "# \treturn f\"### {title}\\n\\n{p_joined}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "# parent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "dir_over = os.path.join(parent_dir, 'hotpotqa')\n",
    "hotpot_dir = os.path.join(parent_dir, 'hotpotqa')\n",
    "triviaqa_dir = os.path.join(parent_dir, 'triviaqa_wikipedia')\n",
    "geval_prompt_dir = os.path.join(parent_dir, 'geval_prompts')\n",
    "\n",
    "print(current_dir)\n",
    "print(parent_dir)\n",
    "print(dir_over)\n",
    "\n",
    "\n",
    "GEVAL_PROMPTS = {}\n",
    "\n",
    "for (prompt_id, path) in [\n",
    "\t(\"coherence\", \"coh_detailed.txt\"),\n",
    "\t(\"consistency\", \"con_detailed.txt\"),\n",
    "\t(\"fluency\", \"flu_detailed.txt\"),\n",
    "\t(\"relevance\", \"rel_detailed.txt\")\n",
    "]:\n",
    "\twith open(os.path.join(geval_prompt_dir, path), \"r\") as f:\n",
    "\t\tGEVAL_PROMPTS[prompt_id] = f.read()\n",
    "\t\tf.close()\n",
    "\n",
    "if not DATASET_SPECS.holdout is None:\n",
    "\twith open(DATASET_SPECS.holdout, \"r\") as f:\n",
    "\t\tHOLDOUT_SET = json.load(f)\n",
    "\t\tf.close()\n",
    "\tHOLDOUT_SET = set(HOLDOUT_SET)\n",
    "else:\n",
    "\tHOLDOUT_SET = set()\n",
    "\n",
    "holdout_count = 0\n",
    "\n",
    "evaluation_questions, dataset_hash = [], \"abc\"\n",
    "\n",
    "if DATASET_SPECS.set == \"hotpotqa\":\n",
    "    # Massive face-palm, we were using the wrong dataset.\n",
    "\t# with open(os.path.join(hotpot_dir, 'hotpot_train_v1.1.json'), 'r') as f:\n",
    "\t# \tHOTPOT_ALL = json.load(f)\n",
    "\t# \tf.close()\n",
    " \n",
    "\twith open(os.path.join(hotpot_dir, 'hotpot_train_v1.1.json'), 'r') as f:\n",
    "\t\tHOTPOT_ALL = json.load(f)\n",
    "\t\tf.close()\n",
    "\t\n",
    "\tnp.random.seed(DATASET_SPECS.seed)\n",
    "\tnp.random.shuffle(HOTPOT_ALL)\n",
    "\t\n",
    "\tall_levels = set()\n",
    "\tfor q in HOTPOT_ALL:\n",
    "\t\tall_levels.add(q[\"level\"])\n",
    "\t\n",
    "\tprint(\"All categories for hotpotqa:\", list(all_levels))\n",
    "\t\n",
    "\tif DATASET_SPECS.subset:\n",
    "\t\tHOTPOT_ALL = [x for x in HOTPOT_ALL if x[\"level\"] == DATASET_SPECS.subset]\n",
    "\t\n",
    "\tHOTPOT_ALL = HOTPOT_ALL[:DATASET_SPECS.samples*2]\n",
    "\t\n",
    "\t\n",
    "\tfor question in tqdm(HOTPOT_ALL):\t\n",
    "\t\tquestion_hash = hash_string(question[\"question\"])\n",
    "     \n",
    "\t\tif question_hash in HOLDOUT_SET:\n",
    "\t\t\tholdout_count += 1\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t\n",
    "\t\tevaluation_questions.append({\n",
    "\t\t\t\"question\": question[\"question\"],\n",
    "\t\t\t\"answer\": question[\"answer\"],\n",
    "\t\t\t\"pages\": [{\n",
    "\t\t\t\t\"title\": e[0],\n",
    "       \t\t\t\"content\": \"\\n\".join(e[1])\n",
    "          \t} for e in question[\"context\"]]\n",
    "\t\t})\n",
    "\n",
    "\t\tdataset_hash = hash_string(f\"{dataset_hash} - {json.dumps(evaluation_questions[-1])}\")\n",
    "\t\t\n",
    "\t\t# evaluation_questions.append(question)\n",
    "elif DATASET_SPECS.set == \"triviaqa\":\n",
    "\tparquet_file_paths = [\n",
    "\t\tf'trivia_qa_wiki_{i}.parquet'\n",
    "\t\tfor i in range(1, 8)\n",
    "\t\t# for i in range(1, 2)\n",
    "\t]\n",
    "\n",
    "\ttotal_scanned = 0\n",
    "\tALL_TRIVIAQA = []\n",
    "\tevaluation_questions = []\n",
    "\tfor parquet_file_path in tqdm(parquet_file_paths):\n",
    "\t\t# Use pandas to read the parquet file\n",
    "\t\tdf = pd.read_parquet(os.path.join(triviaqa_dir, parquet_file_path))\n",
    "\n",
    "\t\t# Display the first few rows of the dataframe\n",
    "\t\t# print(df.head())\n",
    "\n",
    "\t\tfor index, row in df.iterrows():\n",
    "\t\t\tALL_TRIVIAQA.append(row.to_dict())\n",
    "\n",
    "    \n",
    "\tnp.random.seed(DATASET_SPECS.seed)\n",
    "\tnp.random.shuffle(ALL_TRIVIAQA)\n",
    "\tALL_TRIVIAQA = ALL_TRIVIAQA[:DATASET_SPECS.samples*2]\n",
    "\tfor row in ALL_TRIVIAQA:\n",
    "\t\tquestion_hash = hash_string(row[\"question\"])\n",
    "\n",
    "\t\tif question_hash in HOLDOUT_SET:\n",
    "\t\t\tholdout_count += 1\n",
    "\t\t\tcontinue\n",
    "  \n",
    "     \n",
    "\t\tevaluation_questions.append({\n",
    "\t\t\t\"question\": row[\"question\"],\n",
    "\t\t\t# \"answer\": row[\"answer\"],\n",
    "\t\t\t\"pages\": [{\n",
    "\t\t\t\t\"title\": hash_string(e),\n",
    "\t\t\t\t\"content\": e\n",
    "         \t} for e in row[\"entity_pages\"][\"wiki_context\"].tolist()]\n",
    "\t\t})\n",
    "\t\t\n",
    "\t\tdataset_hash = hash_string(f\"{dataset_hash} - {json.dumps(evaluation_questions[-1])}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset set\")\n",
    "\n",
    "print(f\"Dataset Hash: {dataset_hash}\")\n",
    "\n",
    "# if \"dataset_integrity\" in EVAL_RESULTS and EVAL_RESULTS[\"dataset_integrity\"] != dataset_hash:\n",
    "# \traise ValueError(f\"Dataset integrity check failed; hash mismatch: {EVAL_RESULTS['dataset_integrity']} != {dataset_hash}\")\n",
    "\n",
    "EVAL_RESULTS[\"dataset_integrity\"] = dataset_hash\n",
    "\n",
    "print(f\"Total holdouts: {holdout_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from typing import Literal\n",
    "\n",
    "total_input_tokens = {}\n",
    "total_output_tokens = {}\n",
    "\n",
    "rates_per_million = {\n",
    "    \"deepinfra/meta-llama/Llama-3.3-70B-Instruct\": {\"input\": 0.23, \"output\": 0.40},\n",
    "    \"deepinfra/meta-llama/Llama-3.3-70B-Instruct-Turbo\": {\"input\": 0.12, \"output\": 0.30},\n",
    "\t\"deepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct\": {\"input\": 0.23, \"output\": 0.40},\n",
    "\t\"deepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct\": {\"input\": 0.03, \"output\": 0.05},\n",
    "\t\"openai/gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "\t\"openai/gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "}\n",
    "\n",
    "def get_money_spent():\n",
    "\ttotal_cost = 0\n",
    "\tfor model, tokens in total_input_tokens.items():\n",
    "\t\tif model in rates_per_million:\n",
    "\t\t\trate = rates_per_million[model][\"input\"]\n",
    "\t\t\ttotal_cost += tokens * rate / 1000000\n",
    "\tfor model, tokens in total_output_tokens.items():\n",
    "\t\tif model in rates_per_million:\n",
    "\t\t\trate = rates_per_million[model][\"output\"]\n",
    "\t\t\ttotal_cost += tokens * rate / 1000000\n",
    "\treturn total_cost\n",
    "\n",
    "\n",
    "def call_llm_external(chat_history, model_parameters, functions_available=None, provider : Literal[\"openai\", \"deep_infra\"] = \"deep_infra\"):\n",
    "\tglobal total_input_tokens, total_output_tokens\n",
    "\tassert isinstance(model_parameters, dict), \"model_parameters must be a dictionary\"\n",
    "\tassert isinstance(chat_history, list), \"chat_history must be a list\"\n",
    "\tassert all([\"content\" in e and \"role\" in e and e[\"role\"] in [\"user\", \"assistant\", \"system\"] for e in chat_history]), \\\n",
    "\t\t\"chat_history must be a list of dictionaries with 'content' and 'role' keys\"\n",
    "\t\n",
    "\tmodel = model_parameters[\"model\"]\n",
    " \n",
    "\tql_parameters = {k: v for k, v in model_parameters.items() if k not in [\"model\"]}\n",
    " \n",
    "\tresponse = requests.get(f\"http://localhost:8000/api/format_chat_history\", json={\n",
    "\t\t\"auth\": {\"api_key\": DATASET_SPECS.ql_api_key}, \n",
    "\t\t\"chat_history\": chat_history,\n",
    "\t\t**({\"functions_available\": functions_available} if functions_available is not None else {})\n",
    "\t})\n",
    "\tresponse.raise_for_status()\n",
    "\tresult = response.json()\n",
    " \n",
    "\t# print(\"RESULT_PROMPT:\", json.dumps(result, indent=4))\n",
    "\tassert not (\"success\" in result and result[\"success\"] == False), result[\"error\"]\n",
    "\t\n",
    "\tchat_history_chopped = result[\"result\"]\n",
    "\t\n",
    "\t# print(\"CHAT HISTORY CHOPPED:\", json.dumps(chat_history_chopped, indent=4))\n",
    "\t\n",
    "\turl_basis = {\n",
    "\t\t\"deep_infra\": {\"base_url\": \"https://api.deepinfra.com/v1/openai\"},\n",
    "\t\t\"openai\": {}\n",
    "\t}\n",
    "\t\n",
    "\tapi_key_lookup = {\n",
    "\t\t\"deep_infra\": DATASET_SPECS.deep_infra_api_key,\n",
    "\t\t\"openai\": DATASET_SPECS.openai_api_key\n",
    "\t}\n",
    " \n",
    "\topenai = OpenAI(\n",
    "\t\tapi_key=api_key_lookup[provider],\n",
    "\t\t**url_basis[provider]\n",
    "\t)\n",
    "\t\n",
    "\tchat_completion = openai.chat.completions.create(\n",
    "\t\tmodel=model,\n",
    "\t\tmessages=chat_history_chopped,\n",
    "\t\tstop=['<|eot_id|>'],\n",
    "\t\tmax_tokens=model_parameters.get(\"max_tokens\", 100),\n",
    "\t\ttemperature=model_parameters.get(\"temperature\", 0.0),\n",
    "\t\ttop_p=model_parameters.get(\"top_p\", 1.0),\n",
    "\t)\n",
    " \n",
    "\ttoken_model = (\"openai\" if provider == \"openai\" else \"deepinfra\") + \"/\" + model\n",
    " \n",
    "\tif not token_model in total_input_tokens:\n",
    "\t\ttotal_input_tokens[token_model] = 0\n",
    "\tif not token_model in total_output_tokens:\n",
    "\t\ttotal_output_tokens[token_model] = 0\n",
    "\t\n",
    "\t\n",
    "\tresult_message = chat_completion.choices[0].message.content\n",
    "\tprompt_tokens, out_tokens = chat_completion.usage.prompt_tokens, chat_completion.usage.completion_tokens\n",
    "\ttotal_input_tokens[token_model] += prompt_tokens\n",
    "\ttotal_output_tokens[token_model] += out_tokens\n",
    " \n",
    "\tcall_results = {}\n",
    "\tif not functions_available is None:\n",
    "\t\tresponse_2 = requests.get(f\"http://localhost:8000/api/find_function_calls\", json={\n",
    "\t\t\t\"auth\": {\"api_key\": DATASET_SPECS.ql_api_key}, \n",
    "\t\t\t\"text_in\": result_message,\n",
    "\t\t})\n",
    "\t\tresponse_2.raise_for_status()\n",
    "\t\tresult_2 = response_2.json()\n",
    "\t\tassert not (\"success\" in result_2 and result_2[\"success\"] == False), result_2[\"error\"]\n",
    "  \n",
    "\t\t# print(\"FIND FUNCTION CALLS RESULTS:\", json.dumps(result_2, indent=4))\n",
    "\t\tcalls = result_2[\"result\"]\n",
    "\t\tcalls_possible = [\n",
    "\t\t\te[\"name\"] \n",
    "\t\t\tfor e in functions_available \n",
    "\t\t]\n",
    "\t\tcalls = [e for e in calls if \"function\" in e and e[\"function\"] in calls_possible]\n",
    "\t\tcall_results = {\"function_calls\": calls}\n",
    "\t\n",
    "\treturn {\"output\": result_message, **call_results}\n",
    "\n",
    "\n",
    "def call_llm_querylake(chat_history, model_parameters, functions_available=None):\n",
    "\t# global total_input_tokens, total_output_tokens\n",
    "\t\n",
    "\tmodel = model_parameters[\"model\"]\n",
    " \n",
    "\tassert isinstance(model_parameters, dict), \"model_parameters must be a dictionary\"\n",
    "\tassert isinstance(chat_history, list), \"chat_history must be a list\"\n",
    "\tassert all([\"content\" in e and \"role\" in e and e[\"role\"] in [\"user\", \"assistant\", \"system\"] for e in chat_history]), \\\n",
    "\t\t\"chat_history must be a list of dictionaries with 'content' and 'role' keys\"\n",
    "\n",
    "\tresponse = requests.get(f\"http://localhost:8000/api/llm\", json={\n",
    "\t\t\"auth\": {\"api_key\": DATASET_SPECS.ql_api_key}, \n",
    "\t\t\"chat_history\": chat_history,\n",
    "\t\t\"model_parameters\": model_parameters,\n",
    "\t\t**({\"functions_available\": functions_available} if functions_available is not None else {})\n",
    "\t})\n",
    "\tresponse.raise_for_status()\n",
    "\n",
    "\tresult = response.json()\n",
    "\n",
    "\t# print(\"RESULT KEYS:\", result[\"result\"].keys())\n",
    "\n",
    "\tassert \"input_token_count\" in result[\"result\"], \"input_token_count not in result\"\n",
    "\tassert \"output_token_count\" in result[\"result\"], \"output_token_count not in result\"\n",
    " \n",
    "\t# print(\"INPUT TOKENS:\", result[\"result\"][\"input_token_count\"])\n",
    "\t# print(\"OUTPUT TOKENS:\", result[\"result\"][\"output_token_count\"])\n",
    "\n",
    "\tmodel = model_parameters[\"model\"]\n",
    "\t\n",
    "\ttotal_input_tokens[model] = total_input_tokens.get(model, 0) + result[\"result\"][\"input_token_count\"]\n",
    "\ttotal_output_tokens[model] = total_output_tokens.get(model, 0) + result[\"result\"][\"output_token_count\"]\n",
    " \n",
    " \n",
    "\tif (\"success\" in result and result[\"success\"] == False):\n",
    "\t\tprint(json.dumps(result, indent=4))\n",
    "\n",
    "\ttry:\n",
    "\t\tassert not (\"success\" in result and result[\"success\"] == False), result[\"error\"]\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\t\traise e\n",
    "\n",
    "\t# total_input_tokens += result[\"result\"][\"input_token_count\"]\n",
    "\t# total_output_tokens += result[\"result\"][\"output_token_count\"]\n",
    "\n",
    "\treturn result[\"result\"]\n",
    "\n",
    "\n",
    "def call_llm(chat_history, \n",
    "             model_parameters, \n",
    "             functions_available=None):\n",
    "\t\n",
    "\tmodel = model_parameters[\"model\"]\n",
    "\t\n",
    "\t\n",
    " \n",
    "\tmodel_split = model.split(\"/\")\n",
    "\tmodel_main = \"/\".join(model_split[1:]) if len(model_split) > 1 else model_split[0]\n",
    " \n",
    " \n",
    "\tif len(model_split) > 1 and model_split[0] in [\"openai\", \"deepinfra\"]:\n",
    "\t\treturn call_llm_external(\n",
    "      \t\tchat_history, \n",
    "        \t{**model_parameters, \"model\": model_main}, \n",
    "         \tfunctions_available, \n",
    "          \tprovider=\"openai\" if model_split[0] == \"openai\" else \"deep_infra\"\n",
    "        )\n",
    " \n",
    " \n",
    "\t# print(\"CALL_LLM:\", endpoint)\n",
    "\t\n",
    "\treturn call_llm_querylake(chat_history, model_parameters, functions_available)\n",
    "\n",
    "def querylake_chop_chat_history(chat_history, model_parameters, functions_available=None):\n",
    "\tassert isinstance(model_parameters, dict), \"model_parameters must be a dictionary\"\n",
    "\tassert isinstance(chat_history, list), \"chat_history must be a list\"\n",
    "\tassert all([\"content\" in e and \"role\" in e and e[\"role\"] in [\"user\", \"assistant\", \"system\"] for e in chat_history]), \\\n",
    "\t\t\"chat_history must be a list of dictionaries with 'content' and 'role' keys\"\n",
    "\t\n",
    "\tql_parameters = {k: v for k, v in model_parameters.items() if k not in [\"model\"]}\n",
    " \n",
    "\tresponse = requests.get(f\"http://localhost:8000/api/llm\", json={\n",
    "\t\t\"auth\": {\"api_key\": DATASET_SPECS.ql_api_key}, \n",
    "\t\t\"chat_history\": chat_history,\n",
    "\t\t\"model_parameters\": ql_parameters,\n",
    "  \t\t\"only_format_prompt\": True,\n",
    "\t\t**({\"functions_available\": functions_available} if functions_available is not None else {})\n",
    "\t})\n",
    "\tresponse.raise_for_status()\n",
    "\tresult = response.json()\n",
    " \n",
    "\t# print(\"RESULT_PROMPT:\", json.dumps(result, indent=4))\n",
    "\tassert not (\"success\" in result and result[\"success\"] == False), result[\"error\"]\n",
    "\t\n",
    "\tchat_history_chopped = result[\"result\"][\"chat_history\"]\n",
    " \n",
    "\treturn chat_history_chopped\n",
    "\n",
    "def call_search_hybrid(parameters):\n",
    "    \n",
    "    response = requests.get(f\"http://localhost:8000/api/search_hybrid\", json={\n",
    "\t\t\"auth\": {\"api_key\": DATASET_SPECS.ql_api_key},\n",
    "\t\t**parameters\n",
    "\t})\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    result = response.json()\n",
    "    \n",
    "    assert not (\"success\" in result and result[\"success\"] == False), result[\"error\"]\n",
    "    \n",
    "    return result[\"result\"]\n",
    "\n",
    "def call_search_bm25(parameters):\n",
    "    \n",
    "    response = requests.get(f\"http://localhost:8000/api/search_bm25\", json={\n",
    "\t\t\"auth\": {\"api_key\": DATASET_SPECS.ql_api_key},\n",
    "\t\t**parameters\n",
    "\t})\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    result = response.json()\n",
    "    \n",
    "    assert not (\"success\" in result and result[\"success\"] == False), result[\"error\"]\n",
    "    \n",
    "    return result[\"result\"]\n",
    "\n",
    "def count_tokens(input : str):\n",
    "\t\n",
    "\t\n",
    "\tresponse = requests.get(f\"http://localhost:8000/api/llm_count_tokens\", json={\n",
    "\t\t\"auth\": {\"api_key\": DATASET_SPECS.ql_api_key},\n",
    "\t\t\"model_id\": \"llama-3.1-8b-instruct\",\n",
    "\t\t\"input_string\": input\n",
    "\t})\n",
    "\tresponse.raise_for_status()\n",
    "\tresult = response.json()\n",
    " \n",
    "\t# print(\"RESULT_PROMPT:\", json.dumps(result, indent=4))\n",
    "\tassert not (\"success\" in result and result[\"success\"] == False), result[\"error\"]\n",
    "\t\n",
    "\t\n",
    "\treturn result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def call_function_batch(function_in, tuple_args):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(lambda p: function_in(*p), tuple_args))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Any, Union, Awaitable, List, Dict, Tuple\n",
    "import time\n",
    "\n",
    "class DocumentChunkDictionary(BaseModel):\n",
    "    id: Union[str, int, List[str], List[int]]\n",
    "    creation_timestamp: float\n",
    "    collection_type: Optional[Union[str, None]]\n",
    "    document_id: Optional[Union[str, None]]\n",
    "    document_chunk_number: Optional[Union[int, Tuple[int, int], None]]\n",
    "    # document_integrity: Optional[Union[str, None]]\n",
    "    collection_id: Optional[Union[str, None]]\n",
    "    document_name: str\n",
    "    # website_url : Optional[Union[str, None]]\n",
    "    # private: bool\n",
    "    md: dict\n",
    "    document_md: dict\n",
    "    text: str\n",
    "    embedding: Optional[List[float]] = None\n",
    "    \n",
    "    hybrid_score: Optional[float] = None\n",
    "    bm25_score: Optional[float] = None\n",
    "    similarity_score: Optional[float] = None\n",
    "\n",
    "TRAINING_SYSTEM_PROMPT = \"\"\"\n",
    "You are tasked with searching for information to answer a question.\n",
    "You will be given a question, and you must perform searches to find the information necessary to answer it.\n",
    "DO NOT answer with information not provided by searches.\n",
    "DO NOT make up information.\n",
    "ONLY answer with information you have found in the database.\n",
    "\"\"\"\n",
    "\n",
    "TRAINING_PROMPT_1 = \"\"\"\n",
    "You are showing how to do a sample exam to prepare students in a research class.\n",
    "You will be given a question on a topic.\n",
    "You must attempt to answer it by performing consecutive searches and retrieving sources until you are ready to answer.\n",
    "You must perform these searches and make notes of the information as you parse through it until you feel confident that you can answer the question.\n",
    "When you perform a search or otherwise call a function, you will be met with the result as a response. You may then continue.\n",
    "Only respond with your next step.\n",
    "Complete the process in an ideal way by calling provided functions.\n",
    "Your functions will be search_database() for searching, ready_to_answer() for when you decide to answer, and cannot_answer() for if you feel you couldn't answer effectively.\n",
    "Take as many searches as you need, and feel free to try different angles to gain insight.. \n",
    "Only respond with your reasoning and actions.\n",
    "\n",
    "Here is your question: {question}\n",
    "\n",
    "Only respond with your next step as if you were taking the exam.\n",
    "While searching, provide some brief reasoning or observations before performing the search if helpful.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "TRAINING_PROMPT_2 = \"\"\"\n",
    "You will be given a question on a topic requiring 1-2 paragraphs to answer\n",
    "You must attempt to answer it by performing consecutive searches and retrieving sources until you are ready to answer.\n",
    "You must perform these searches and make notes of the information as you parse through it until you feel confident that you can answer the question.\n",
    "When you perform a search or otherwise call a function, you will be met with the result as a response. You may then continue.\n",
    "Only respond with your next step.\n",
    "Complete the process in an ideal way by calling provided functions.\n",
    "Your functions will be search_database() for searching, ready_to_answer() to indicate that you are ready to answer, and cannot_answer() for if you feel you couldn't answer effectively.\n",
    "You are allowed {max_search_string} before answering, no more.\n",
    "Only respond with your reasoning and actions.\n",
    "\n",
    "Here is your question: {question}\n",
    "\n",
    "Only respond with your next step.\n",
    "\"\"\"\n",
    "\n",
    "answer_enabled_prompt = \"\"\"\n",
    "Answering enabled. Go ahead and write your final answer, and nothing else. \n",
    "Please cite your sources with frequent inline citations whenever they are even slightly applicable.\n",
    "Use the citations provided inside the <CITATION> XML of respective sources (i.e. {cite:1}, do not include the XML tags).\n",
    "Again, citations should be as frequent as possible. While claiming something, \n",
    "you should ideally have an appropriate citation at the end of every sentence.\n",
    "Do not make any claims not supported by the sources provided.\n",
    "Continue.\n",
    "\"\"\"\n",
    "\n",
    "could_not_answer_prompt = \"\"\"\n",
    "You have indicated that you cannot answer.\n",
    "Please provide a brief explanation as to why you cannot answer the question.\n",
    "Also, provide what information you have uncovered. \n",
    "Please cite your sources with frequent inline citations whenever they are applicable.\n",
    "Use the citations provided inside the <CITATION> XML of respective sources (i.e. {cite:1}, do not include the XML tags).\n",
    "Continue.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "search_func_description = \"\"\"\n",
    "Perform a search of the database for information.\n",
    "This is deterministic, so don't repeat the same search.\n",
    "\"\"\"\n",
    "\n",
    "search_func_def = {\n",
    "    \"name\": \"search_database\",\n",
    "    \"description\": search_func_description,\n",
    "    \"parameters\": [\n",
    "        {\n",
    "            \"name\": \"question\",\n",
    "            \"type\": \"str\",\n",
    "            \"description\": \"Effectively a google search. Will be used to retrieve information from the database via term similarity.\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "answer_func_def = {\n",
    "    \"name\": \"ready_to_answer\",\n",
    "    \"description\": \"\"\"\n",
    "If you feel you have enough information to answer the question, call this to let the system know you are ready to answer.\n",
    "Only then can you write your answer.\n",
    "\"\"\",\n",
    "    \"parameters\": []\n",
    "}\n",
    "\n",
    "cannot_answer_func_def = {\n",
    "    \"name\": \"cannot_answer\",\n",
    "    \"description\": \"If you feel you are unable to answer the question even with the tools available, call this.\",\n",
    "    \"parameters\": []\n",
    "}\n",
    "\n",
    "\n",
    "def self_guided_search_manual(\n",
    "    model : str,\n",
    "    question: str,\n",
    "    collection_ids: List[str] = [],\n",
    "    max_searches : int = 5,\n",
    "    use_hybrid: bool = False,\n",
    "    use_rerank: int = None,\n",
    "    use_metadata: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Self guided search.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    answer_flag, answer, sources_matched, responses, searches = False, None, [], 0, 0\n",
    "    ready_to_answer_flag = False\n",
    "    demo_sequence = []\n",
    "    max_search_string = f\"{max_searches} searches\" if max_searches > 1 else \"1 search\"\n",
    "    chat_history_1 = [\n",
    "        {\"role\": \"system\", \"content\": TRAINING_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": TRAINING_PROMPT_2.format(question=question, max_search_string=max_search_string)}\n",
    "    ]\n",
    "\n",
    "    max_responses, prompt_tokens, output_tokens = 15, 0, 0\n",
    "\n",
    "    previous_searches, previous_results, all_sources = set(), [], []\n",
    "    answer_found = False\n",
    "    \n",
    "    searches_return = []\n",
    "    \n",
    "    \n",
    "    def on_new_search(search: str):\n",
    "        nonlocal searches_return\n",
    "        searches_return.append({\"search\": search})\n",
    "                \n",
    "    def on_new_source(source_in: dict):\n",
    "        nonlocal all_sources\n",
    "        all_sources.append(source_in)\n",
    "    \n",
    "    while True:\n",
    "        if ready_to_answer_flag:\n",
    "            answer_flag = True\n",
    "        \n",
    "        model_parameters = {\n",
    "            \"model\": model,\n",
    "            \"max_tokens\": 200 if not answer_flag else 4096,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"repetition_penalty\": 1.15\n",
    "        }\n",
    "        # print(\"Looping with chat history:\", [small_map[chat[\"role\"]] for chat in chat_history_1])\n",
    "        all_functions_available = [answer_func_def, cannot_answer_func_def]\n",
    "        last_statement = \"\"\n",
    "        if searches < max_searches and not answer_flag:\n",
    "            all_functions_available.append(search_func_def)\n",
    "            last_statement = f\"\\n\\nYou have {max_searches - searches} searches remaining.\"\n",
    "        elif searches >= max_searches and not answer_flag:\n",
    "            # answer_flag = True\n",
    "            last_statement = \"\\n\\n-------ATTENTION------- You have no searches remaining.\"\n",
    "        \n",
    "        chat_history_1[-1][\"content\"] += last_statement\n",
    "        \n",
    "\n",
    "        \n",
    "        model_response = call_llm(\n",
    "            chat_history=chat_history_1, \n",
    "            model_parameters=model_parameters,\n",
    "            functions_available=all_functions_available if not answer_flag else [],\n",
    "        )\n",
    "        \n",
    "        prompt_tokens += model_response.get(\"input_token_count\", 0)\n",
    "        output_tokens += model_response.get(\"output_token_count\", 0)\n",
    "        \n",
    "\n",
    "        responses += 1\n",
    "        chat_history_1.append({\"role\": \"assistant\", \"content\": model_response[\"output\"], \"function_calls\": model_response.get(\"function_calls\", [])})\n",
    "        demo_sequence.append({\"role\": \"assistant\", \"content\": model_response[\"output\"]})\n",
    "\n",
    "        citation_map = {}\n",
    "        \n",
    "        if \"function_calls\" in model_response and len(model_response[\"function_calls\"]) > 0:\n",
    "            \n",
    "            if model_response[\"function_calls\"][-1][\"function\"] == \"search_database\" and \"question\" in model_response[\"function_calls\"][-1][\"arguments\"]:\n",
    "                new_search = model_response[\"function_calls\"][-1][\"arguments\"][\"question\"]\n",
    "                if new_search in previous_searches:\n",
    "                    response = {\"role\": \"user\", \"content\": \"You have already requested this search. Refer to the results from that attempt. Continue.\"}\n",
    "                    chat_history_1.append(response)\n",
    "                    demo_sequence.append(response)\n",
    "                    continue\n",
    "                \n",
    "                searches += 1\n",
    "                excluded_chunks = \" \".join([f\"-id:\\\"{result}\\\"\" for result in previous_results])\n",
    "\n",
    "                search_make = model_response[\"function_calls\"][-1][\"arguments\"][\"question\"] + f\" {excluded_chunks}\"\n",
    "                on_new_search(f\"Searching: \\\"{new_search}\\\"\")\n",
    "                \n",
    "                # print(\"PREVIOUS RESULTS:\", previous_results)\n",
    "                # print(\"SEARCH MADE:\", search_make)\n",
    "                if not use_hybrid:\n",
    "                    searched_sources : List[DocumentChunkDictionary] = call_search_bm25(dict(\n",
    "                        query=search_make,\n",
    "                        collection_ids=collection_ids,\n",
    "                        limit=5,\n",
    "                    ))\n",
    "                else:\n",
    "                    split_size = 5 if use_rerank is None else use_rerank // 2\n",
    "                    \n",
    "                    searched_sources : List[DocumentChunkDictionary] = call_search_hybrid(dict(\n",
    "                        query=search_make,\n",
    "                        collection_ids=collection_ids,\n",
    "                        limit_bm25=split_size,\n",
    "                        limit_similarity=split_size,\n",
    "                        rerank=(True if not use_rerank is None else False)\n",
    "                    ))\n",
    "                    searched_sources = searched_sources[:5]\n",
    "                    \n",
    "                \n",
    "                \n",
    "                for source in searched_sources:\n",
    "                    on_new_source(source)\n",
    "                \n",
    "                for i in range(len(searched_sources)):\n",
    "                    citation_map[\n",
    "                        searched_sources[i][\"id\"] \n",
    "                        if isinstance(searched_sources[i][\"id\"], str) \n",
    "                        else searched_sources[i][\"id\"][0]\n",
    "                    ] = len(sources_matched) + i + 1\n",
    "                \n",
    "                for source in searched_sources:\n",
    "                    if isinstance(source[\"id\"], list):\n",
    "                        sources_matched.extend(source[\"id\"])\n",
    "                    else:\n",
    "                        sources_matched.append(source[\"id\"])\n",
    "                        \n",
    "                    \n",
    "                \n",
    "                sources_represented = [\n",
    "                    \"<CITATION>\\n\\t{cite:\" + str(citation_map[\n",
    "                        source[\"id\"]\n",
    "                        if isinstance(source[\"id\"], str) \n",
    "                        else source[\"id\"][0]\n",
    "                    ]) + \"}\\n</CITATION>\\n\" + \\\n",
    "                    f\"<CONTENT>\\n{source['text']}\\n</CONTENT>\" + \\\n",
    "                    ((\"\\n<METADATA>\\n\" +\n",
    "                    json.dumps({\n",
    "                        k: v for k, v in source.items() \n",
    "                        if k not in [\"embedding\", \"collection_id\", \"collection_type\", \"creation_timestamp\", \"text\"]    \n",
    "                    }, indent=4)  + \"\\n</METADATA>\") if use_metadata else \"\")\n",
    "                    for source in searched_sources\n",
    "                ]\n",
    "                searched_sources_string = \"\\n\\n\".join([source_repr for source_repr in sources_represented])\n",
    "                # print(\"SEARCHED SOURCES:\", searched_sources)\n",
    "                response = {\"role\": \"user\", \"content\": f\"<SEARCH_RESULTS>\\n{searched_sources_string}\\n</SEARCH_RESULTS>\"}\n",
    "                chat_history_1.append(response)\n",
    "                demo_sequence.append({**response, \"sources\": searched_sources})\n",
    "                previous_searches.add(new_search)\n",
    "                previous_results.extend([source[\"id\"] for source in searched_sources])\n",
    "            \n",
    "            elif model_response[\"function_calls\"][-1][\"function\"] == \"ready_to_answer\":\n",
    "\n",
    "                # print(\"Got function calls:\", model_response[\"function_calls\"])\n",
    "                # if len(model_response[\"output\"].split(\"&& > ready_to_answer() &&\")[-1]) > 45:\n",
    "                #     answer = model_response[\"output\"].split(\"&& > ready_to_answer() &&\")[-1]\n",
    "                #     break\n",
    "                response = {\"role\": \"user\", \"content\": answer_enabled_prompt}\n",
    "                chat_history_1.append(response)\n",
    "                demo_sequence.append(response)\n",
    "                # print(\"Getting ready to answer.\")\n",
    "                ready_to_answer_flag = True\n",
    "                answer_flag = False\n",
    "                answer_found = True\n",
    "\n",
    "            elif model_response[\"function_calls\"][-1][\"function\"] == \"cannot_answer\":\n",
    "                # if len(model_response[\"output\"].split(\"&& > ready_to_answer() &&\")[-1]) > 45:\n",
    "                #     answer = model_response[\"output\"].split(\"&& > ready_to_answer() &&\")[-1]\n",
    "                #     break\n",
    "                response = {\"role\": \"user\", \"content\": could_not_answer_prompt}\n",
    "                chat_history_1.append(response)\n",
    "                demo_sequence.append(response)\n",
    "                # print(\"Getting ready to answer.\")\n",
    "                ready_to_answer_flag = True\n",
    "                answer_flag = False\n",
    "        \n",
    "        if answer_flag:\n",
    "            answer = model_response[\"output\"]\n",
    "            break\n",
    "        \n",
    "        if chat_history_1[-1][\"role\"] != \"user\":\n",
    "            response = {\"role\": \"user\", \"content\": \"No function calls were parsed. Please remember all function calls must be ended with ' &&'. Continue\"}\n",
    "            chat_history_1.append(response)\n",
    "            demo_sequence.append(response)\n",
    "\n",
    "        if responses >= max_responses:\n",
    "            max_cite_index = max([int(cite.split(\":\")[-1][:-1]) for cite in citation_map.keys()])\n",
    "            assert max_cite_index == len(sources_matched), f\"Max cite index: {max_cite_index}, sources matched: {len(sources_matched)}, citation map: {citation_map}\"\n",
    "            \n",
    "            return {\n",
    "                \"chat_history\": chat_history_1, \n",
    "                \"output\": \"Model ran out of responses.\", \n",
    "                \"responses\": responses, \n",
    "                \"time_taken\": time.time() - start_time,\n",
    "                \"sources\": [], \n",
    "                \"answer_found\": answer_found,\n",
    "                \"searches\": searches_return,\n",
    "                \"input_token_count\": prompt_tokens,\n",
    "                \"output_token_count\": output_tokens\n",
    "            }\n",
    "\n",
    "\n",
    "    # print(\"RETURNING DEMO SEQUENCE WITH LENGTH\", len(demo_sequence))\n",
    "    return {\n",
    "        \"chat_history\": chat_history_1, \n",
    "        \"output\": answer,\n",
    "        \"responses\": responses,\n",
    "        \"time_taken\": time.time() - start_time,\n",
    "        \"sources\": all_sources,\n",
    "        \"answer_found\": answer_found,\n",
    "        \"searches\": searches_return,\n",
    "        \"input_token_count\": prompt_tokens,\n",
    "        \"output_token_count\": output_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_create_correct_answer = \"\"\"\n",
    "You are an assistant that answers user questions, but you only do so using information sources provided along with requests.\n",
    "\"\"\"\n",
    "\n",
    "prompt_create_correct_answer = \"\"\"\n",
    "Answer the following questions using the provided sources.\n",
    "If you cannot answer the question, simply write CANNOT_ANSWER.\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\n",
    "<SOURCES>\n",
    "{sources}\n",
    "</SOURCES>\n",
    "\"\"\"\n",
    "\n",
    "for i, entry in tqdm(enumerate(evaluation_questions)):\n",
    "\tquestion_hash = hash_string(entry[\"question\"])\n",
    "    \n",
    "\t# In the case of TriviaQA, the correct answers don't exist,\n",
    "\t# We have to generate it with a model using the correct sources.\n",
    "\t\n",
    "\tif (\"answer\" not in entry) and (question_hash not in CORRECT_ANSWERS_LOOKUP_TRIVIA):\n",
    "\t\tsources = \"\\n\".join([f\"INFORMATION:\\n{e['content']}\\n\\n\" for e in entry[\"pages\"]])\n",
    "\t\tprompt = prompt_create_correct_answer.format(question=entry[\"question\"], sources=sources)\n",
    "\t\t\n",
    "\t\tchat_history = [\n",
    "\t\t\t{\"content\": system_prompt_create_correct_answer, \"role\": \"system\"},\n",
    "\t\t\t{\"content\": prompt, \"role\": \"user\"},\n",
    "\t\t]\n",
    "\t\t\n",
    "\n",
    "\t\tmodel_response = call_llm(\n",
    "\t\t\tchat_history=chat_history,\n",
    "\t\t\tmodel_parameters={\n",
    "\t\t\t\t\"model\": DATASET_SPECS.gt_answer_model,\n",
    "\t\t\t\t\"max_tokens\": 4096,\n",
    "\t\t\t\t\"temperature\": 0.0,\n",
    "\t\t\t\t\"top_p\": 1.0\n",
    "\t\t\t},\n",
    "\t\t\tfunctions_available=None,\n",
    "\t\t)\n",
    "\n",
    "\t\tanswer = model_response[\"output\"]\n",
    "\n",
    "\t\tCORRECT_ANSWERS_LOOKUP_TRIVIA[question_hash] = answer\n",
    "\t\t\n",
    "\tif DATASET_SPECS.set == \"triviaqa\":\n",
    "\t\tevaluation_questions[i][\"answer\"] = CORRECT_ANSWERS_LOOKUP_TRIVIA[question_hash]\n",
    "  \n",
    "evaluation_questions_filtered = [e for e in evaluation_questions if \"answer\" in e and \"CANNOT_ANSWER\" not in e[\"answer\"]]\n",
    "\n",
    "\n",
    "assert len(evaluation_questions_filtered) >= DATASET_SPECS.samples, \\\n",
    "    f\"Only {len(evaluation_questions_filtered)} questions available ({DATASET_SPECS.samples} requested).\"\n",
    "    \n",
    "evaluation_questions = evaluation_questions_filtered[:DATASET_SPECS.samples]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_chunk_hit_rate(chunks, documents):\n",
    "\thits = 0\n",
    "\tfor chunk in chunks:\n",
    "\t\tfor document in documents:\n",
    "\t\t\tif chunk in document:\n",
    "\t\t\t\thits += 1\n",
    "\t\t\t\tbreak\n",
    "\treturn hits / len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Guided Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_MODEL_RESPONSE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_guided_search_bounded(\n",
    "    question : str, \n",
    "    max_searches : int,\n",
    "    use_hybrid : bool = False,\n",
    "    use_rerank : bool = False,\n",
    "    use_context_expansion : bool = False,\n",
    "    rerank_n : int = 100,\n",
    "):\n",
    "\tresponse = requests.post(\"http://localhost:8000/api/self_guided_search\", json={\n",
    "\t\t\"auth\": {\"api_key\": DATASET_SPECS.ql_api_key},\n",
    "\t\t\"question\": question,\n",
    "\t\t\"collection_ids\": TARGET_COLLECTIONS,\n",
    "\t\t\"model\": DATASET_SPECS.model, # TODO: get model\n",
    "\t\t\"max_searches\": max_searches,\n",
    "\t\t\"use_hybrid\": use_hybrid,\n",
    "\t\t**({\"use_rerank\": rerank_n} if use_rerank else {}),\n",
    "\t})\n",
    "\t\n",
    "\tresponse.raise_for_status()\n",
    "\n",
    "\tresult = response.json()\n",
    "\n",
    "\tassert not (\"success\" in result and result[\"success\"] == False), result[\"error\"]\n",
    " \n",
    "\tresult = result[\"result\"]\n",
    "\t\n",
    "\tassert \"input_token_count\" in result, \"input_token_count not in result\"\n",
    "\tassert \"output_token_count\" in result, \"output_token_count not in result\"\n",
    "\t\n",
    "\tmodel = DATASET_SPECS.model\n",
    "\t\n",
    "\ttotal_input_tokens[model] = total_input_tokens.get(model, 0) + result[\"input_token_count\"]\n",
    "\ttotal_output_tokens[model] = total_output_tokens.get(model, 0) + result[\"output_token_count\"]\n",
    "\t\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_check_answer_correctness = \"\"\"\n",
    "You must grade a student's answer for correctness given the correct answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Correct answer: {correct_answer}\n",
    "\n",
    "Student Answer: {student_answer}\n",
    "\n",
    "\n",
    "Was the student answer factually correct?\n",
    "Respond YES or NO, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "cached_amount = 0\n",
    "\n",
    "samples_completed = 0\n",
    "samples_per_question = \\\n",
    "    (1 if DATASET_SPECS.unbounded_cases is None else len(DATASET_SPECS.unbounded_cases)) + \\\n",
    "    (0 if DATASET_SPECS.max_search_depth is None else DATASET_SPECS.max_search_depth)\n",
    "\n",
    "total_samples = DATASET_SPECS.samples * samples_per_question\n",
    "\n",
    "time_started = time.time()\n",
    "\n",
    "def notify_sample_completed():\n",
    "\tglobal samples_completed, total_samples, time_started\n",
    "\tsamples_completed += 1\n",
    "\ttime_elapsed = time.time() - time_started\n",
    " \n",
    "\ttime_per_sample = time_elapsed / samples_completed\n",
    "\ttime_per_question = time_per_sample * samples_per_question\n",
    " \n",
    " \n",
    "\tprint(\"Samples completed: %05d / %05d ( %10.2fs %10.2fs/sample %10.2fs/q )        \" % (\n",
    "     \tsamples_completed, total_samples,\n",
    "\t\ttime_elapsed, time_per_sample, time_per_question\n",
    "    ), end=\"\\r\")\n",
    "\n",
    "\n",
    "def evaluate_question(entry, entry_i, question_hash):\n",
    "\tmodel=DATASET_SPECS.model\n",
    "\trun_successful = False\n",
    "\tretries = 0\n",
    "\t\n",
    "\tif \"error_occurrence\" in entry:\n",
    "\t\treturn\n",
    " \n",
    "\t\t\n",
    "\t\t\n",
    "\t# In the case of TriviaQA, the correct answers don't exist,\n",
    "\t# We have to generate it with a model using the correct sources.\n",
    "\tif (\"answer\" not in entry) and (question_hash not in CORRECT_ANSWERS_LOOKUP_TRIVIA):\n",
    "\t\tsources = \"\\n\".join([f\"INFORMATION:\\n{e}\\n\\n\" for e in entry[\"pages\"]])\n",
    "\t\tprompt = prompt_create_correct_answer.format(question=entry[\"question\"], sources=sources)\n",
    "\t\t\n",
    "\t\tchat_history = [\n",
    "\t\t\t{\"content\": system_prompt_create_correct_answer, \"role\": \"system\"},\n",
    "\t\t\t{\"content\": prompt, \"role\": \"user\"},\n",
    "\t\t]\n",
    "\t\t\n",
    "\t\ttokens = count_tokens(\" \".join([e[\"content\"] for e in chat_history]))\n",
    "\t\t\n",
    "\t\t# print(\"TOKEN COUNT:\", tokens)\n",
    "\n",
    "\t\tmodel_response = call_llm(\n",
    "\t\t\tchat_history=chat_history,\n",
    "\t\t\tmodel_parameters={\n",
    "\t\t\t\t\"model\": DATASET_SPECS.gt_answer_model,\n",
    "\t\t\t\t\"max_tokens\": 4096,\n",
    "\t\t\t\t\"temperature\": 0.0,\n",
    "\t\t\t\t\"top_p\": 1.0\n",
    "\t\t\t},\n",
    "\t\t\tfunctions_available=None,\n",
    "\t\t)\n",
    "\n",
    "\t\tanswer = model_response[\"output\"]\n",
    "\n",
    "\t\tCORRECT_ANSWERS_LOOKUP_TRIVIA[question_hash] = answer\n",
    "\telse:\n",
    "\t\t# cached_amount += 1\n",
    "\t\tpass\n",
    "\t\n",
    "\tif \"answer\" in entry:\n",
    "\t\tcorrect_answer = entry[\"answer\"]\n",
    "\telse:\n",
    "\t\tcorrect_answer = CORRECT_ANSWERS_LOOKUP_TRIVIA[question_hash]\n",
    "\t\n",
    "\tif \"CANNOT_ANSWER\" in correct_answer:\n",
    "\t\treturn\n",
    "\t\n",
    "\t\n",
    "\tif model not in EVAL_RESULTS:\n",
    "\t\tEVAL_RESULTS[model] = {}\n",
    "\t\n",
    "\t\n",
    "\t# Unbounded case\n",
    "\tif not question_hash in EVAL_RESULTS[model]:\n",
    "\t\tEVAL_RESULTS[model][question_hash] = {}\n",
    "\n",
    "\tunbounded_cases = [\"BM25\"] if DATASET_SPECS.unbounded_cases is None else DATASET_SPECS.unbounded_cases\n",
    "\t\n",
    "\tfor unbounded_case in unbounded_cases:\n",
    "\t\tspecs_raw = unbounded_case.split(\"_\")\n",
    "\t\tuse_hybrid_tmp = \"HS\" in specs_raw\n",
    "\t\tassert use_hybrid_tmp == (not \"BM25\" in specs_raw), \"Cannot use both BM25 and HS\"\n",
    "\t\tuse_rerank_tmp = \"RR\" in specs_raw\n",
    "\t\tuse_context_expansion_tmp = \"CE\" in specs_raw\n",
    "\n",
    "\t\tunbounded_case_identifier = f\"unbounded_{unbounded_case}\"\n",
    "\n",
    "\t\tif unbounded_case_identifier in EVAL_RESULTS[model][question_hash]:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t\n",
    "\t\t# Calling through QueryLake\n",
    "\t\t# sgs_result = self_guided_search_bounded(\n",
    "\t\t# \tentry[\"question\"], \n",
    "\t\t# \t99,\n",
    "\t\t# \tuse_hybrid=use_hybrid_tmp,\n",
    "\t\t# \tuse_rerank=use_rerank_tmp,\n",
    "\t\t# \trerank_n=DATASET_SPECS.rerank_n if use_rerank_tmp else None,\n",
    "\t\t# \tuse_context_expansion=use_context_expansion_tmp\n",
    "\t\t# )\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tsgs_result = self_guided_search_manual(\n",
    "\t\t\t\tmodel=DATASET_SPECS.model,\n",
    "\t\t\t\tquestion=entry[\"question\"],\n",
    "\t\t\t\tcollection_ids=TARGET_COLLECTIONS,\n",
    "\t\t\t\tmax_searches=99,\n",
    "\t\t\t\tuse_hybrid=use_hybrid_tmp,\n",
    "\t\t\t\tuse_rerank=DATASET_SPECS.rerank_n if use_rerank_tmp else None,\n",
    "\t\t\t\tuse_metadata=DATASET_SPECS.include_chunk_metadata,\n",
    "\t\t\t\t# use_context_expansion=use_context_expansion_tmp\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tEVAL_RESULTS[model][question_hash][unbounded_case_identifier] = {\n",
    "\t\t\t\t\"error_occurrence\": True\n",
    "\t\t\t}\n",
    "\t\t\tcontinue\n",
    "\t\n",
    "\t\t\n",
    "\t\tchat_sequence, test_answer, duration, response_count = \\\n",
    "\t\t\tsgs_result[\"chat_history\"], sgs_result[\"output\"], \\\n",
    "\t\t\tsgs_result[\"time_taken\"], sgs_result[\"responses\"]\n",
    "\n",
    "\t\tcorrectness_prompt = prompt_check_answer_correctness.format(question=entry[\"question\"], correct_answer=correct_answer, student_answer=test_answer)\n",
    "\t\t# If we haven't already evaluated this answer, go ahead and do so.\n",
    "\t\tmodel_response_check_correctness = call_llm(\n",
    "\t\t\tchat_history=[{\"content\": correctness_prompt, \"role\": \"user\"}],\n",
    "\t\t\tmodel_parameters={\n",
    "\t\t\t\t\"model\": DATASET_SPECS.evaluator_model,\n",
    "\t\t\t\t\"max_tokens\": 4096,\n",
    "\t\t\t\t\"temperature\": 0.0,\n",
    "\t\t\t\t\"top_p\": 1.0\n",
    "\t\t\t},\n",
    "\t\t\tfunctions_available=None\n",
    "\t\t) if test_answer is not None else {\"output\": \"ANSWER_UNAVAILABLE\"}\n",
    "\t\t\n",
    "\t\tnotify_sample_completed()\n",
    "\n",
    "\t\tEVAL_RESULTS[model][question_hash][unbounded_case_identifier] = {\n",
    "\t\t\t\"chat_sequence\": chat_sequence,\n",
    "\t\t\t\"use_hybrid\": use_hybrid_tmp,\n",
    "\t\t\t\"rerank\": DATASET_SPECS.rerank_n if use_rerank_tmp else False,\n",
    "\t\t\t\"context_expansion\": use_context_expansion_tmp,\n",
    "\t\t\t\"question\": entry[\"question\"],\n",
    "\t\t\t\"correct_answer\": correct_answer,\n",
    "\t\t\t\"ground_truth_sources\": entry[\"pages\"],\n",
    "\t\t\t\"retrieved_sources\": sgs_result[\"sources\"],\n",
    "\t\t\t\"test_answer\": test_answer,\n",
    "\t\t\t\"duration\": duration,\n",
    "\t\t\t\"response_count\": response_count,\n",
    "\t\t\t\"correctness_rating\": model_response_check_correctness[\"output\"],\n",
    "\t\t}\n",
    "\n",
    "\tfor max_searches in range(1, DATASET_SPECS.max_search_depth+1) if isinstance(DATASET_SPECS.max_search_depth, int) else []:\n",
    "\t\t\n",
    "\t\t# If we haven't already evaluated this answer, go ahead and do so.\n",
    "\t\tif f\"max_searches_{max_searches}\" in EVAL_RESULTS[model][question_hash]:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Calling through QueryLake\n",
    "\t\t# sgs_result = self_guided_search_bounded(entry[\"question\"], max_searches)\n",
    "\t\ttry:\n",
    "\t\t\tsgs_result = self_guided_search_manual(\n",
    "\t\t\t\tmodel=DATASET_SPECS.model,\n",
    "\t\t\t\tquestion=entry[\"question\"],\n",
    "\t\t\t\tcollection_ids=TARGET_COLLECTIONS,\n",
    "\t\t\t\tmax_searches=max_searches,\n",
    "\t\t\t\tuse_metadata=DATASET_SPECS.include_chunk_metadata,\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tEVAL_RESULTS[model][question_hash][f\"max_searches_{max_searches}\"] = {\n",
    "\t\t\t\t\"error_occurrence\": True\n",
    "\t\t\t}\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tchat_sequence, test_answer, duration, response_count = \\\n",
    "\t\t\tsgs_result[\"chat_history\"], sgs_result[\"output\"], \\\n",
    "\t\t\tsgs_result[\"time_taken\"], sgs_result[\"responses\"]\n",
    "\n",
    "\t\tcorrectness_prompt = prompt_check_answer_correctness.format(question=entry[\"question\"], correct_answer=correct_answer, student_answer=test_answer)\n",
    "\n",
    "\t\tmodel_response_check_correctness = call_llm(\n",
    "\t\t\tchat_history=[{\"content\": correctness_prompt, \"role\": \"user\"}],\n",
    "\t\t\tmodel_parameters={\n",
    "\t\t\t\t\"model\": DATASET_SPECS.evaluator_model,\n",
    "\t\t\t\t\"max_tokens\": 4096,\n",
    "\t\t\t\t\"temperature\": 0.0,\n",
    "\t\t\t\t\"top_p\": 1.0\n",
    "\t\t\t},\n",
    "\t\t\tfunctions_available=None\n",
    "\t\t) if test_answer is not None else {\"output\": \"0\"}\n",
    "\n",
    "\t\t# rating = re.search(r\"(\\d)/5\", model_response_check_correctness[\"output\"])\n",
    "\t\t# if rating is None:\n",
    "\t\t# \trating = 0\n",
    "\t\t# else:\n",
    "\t\t# \trating = int(rating.group(1))\n",
    "\n",
    "\t\tnotify_sample_completed()\n",
    "\n",
    "\t\tEVAL_RESULTS[model][question_hash][f\"max_searches_{max_searches}\"] = {\n",
    "\t\t\t\"chat_sequence\": chat_sequence,\n",
    "\t\t\t\"question\": entry[\"question\"],\n",
    "\t\t\t\"correct_answer\": correct_answer,\n",
    "\t\t\t\"ground_truth_sources\": entry[\"pages\"],\n",
    "\t\t\t\"retrieved_sources\": sgs_result[\"sources\"],\n",
    "\t\t\t\"test_answer\": test_answer,\n",
    "\t\t\t\"duration\": duration,\n",
    "\t\t\t\"response_count\": response_count,\n",
    "\t\t\t\"correctness_rating\": model_response_check_correctness[\"output\"],\n",
    "\t\t}\n",
    "\t\n",
    "\n",
    "\n",
    "\n",
    "\t# if entry_i % 10 == 9:\n",
    "\t# \tprint(\"Total cost $%.4f\" % (get_money_spent()))\n",
    "\n",
    "\trun_successful = True\n",
    "\n",
    "arguments = []\n",
    "\n",
    "for entry_i in tqdm(range(len(evaluation_questions))):\n",
    "# for entry_i in range(len(evaluation_questions)):\n",
    "\t\n",
    "\tentry = evaluation_questions[entry_i]\n",
    "\t# print(\"Evaluating entry:\", entry)\n",
    "\tquestion_hash = hash_string(entry[\"question\"])\n",
    "\t\n",
    "\t# evaluate_question(entry, entry_i, question_hash)\n",
    "\targuments.append((entry, entry_i, question_hash))\n",
    "\tif (entry_i % DATASET_SPECS.batch_size == DATASET_SPECS.batch_size - 1) or entry_i == len(evaluation_questions) - 1:\n",
    "\t\t# print(\"CALLING AT\", entry_i)\n",
    "\t\tcall_function_batch(evaluate_question, arguments)\n",
    "\t\targuments = []\n",
    "\n",
    "\n",
    "print(\"Cache rate: %%%.2f\" % (100 * cached_amount / len(evaluation_questions)))\n",
    "print(\"Total cost $%.4f\" % (get_money_spent()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"correct_answers_lookup_hotpot.json\", \"w\") as f:\n",
    "\tjson.dump(CORRECT_ANSWERS_LOOKUP_HOTPOT, f, indent=4)\n",
    "\tf.close()\n",
    "\n",
    "with open(\"correct_answers_lookup.json\", \"w\") as f:\n",
    "\tjson.dump(CORRECT_ANSWERS_LOOKUP_TRIVIA, f, indent=4)\n",
    "\tf.close()\n",
    "\n",
    "\n",
    "# for entry_i in tqdm(range(len(evaluation_questions[133:]))):\n",
    "# \tquestion_hash = hash_string(entry[\"question\"])\n",
    "\t\n",
    "# \tif question_hash in EVAL_RESULTS[DATASET_SPECS.model]:\n",
    "# \t\tdel EVAL_RESULTS[DATASET_SPECS.model][question_hash]\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "\tjson.dump(EVAL_RESULTS, f, indent=4)\n",
    "\tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys = []\n",
    "RESULTS_BY_TYPE = {}\n",
    "\n",
    "for question_hash, entry in EVAL_RESULTS[DATASET_SPECS.model].items():\n",
    "\tfor key in entry.keys():\n",
    "\t\tif key == \"error_occurrence\":\n",
    "\t\t\tcontinue\n",
    "\t\tif key not in unique_keys:\n",
    "\t\t\tunique_keys.append(key)\n",
    "\t\t\tRESULTS_BY_TYPE[key] = []\n",
    "\t\t# RESULTS_BY_TYPE[key].append(entry[key][\"correctness_rating\"])\n",
    "\n",
    "unique_keys = [f\"max_searches_{i}\" for i in range(1, DATASET_SPECS.max_search_depth+1)] + \\\n",
    "    [\"unbounded_BM25\"]\n",
    "\n",
    "# Only register questions with all tests evaluated, so that test set is properly controlled.\n",
    "for question_hash, entry in EVAL_RESULTS[DATASET_SPECS.model].items():\n",
    "\t# if not all([key in entry and (not \"error_occurrence\" in entry[key]) for key in unique_keys]):\n",
    "\t# \tcontinue\n",
    "\t\n",
    "\tfor key in entry:\n",
    "\t\tif key == \"error_occurrence\":\n",
    "\t\t\tcontinue\n",
    "\t\tif \"error_occurrence\" in entry[key]:\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tassert \"correctness_rating\" in entry[key], f\"correctness_rating not in entry for key {key}\"\n",
    "\t\tRESULTS_BY_TYPE[key].append(entry[key][\"correctness_rating\"])\n",
    "\n",
    "RESULTS_PERCENTAGES = {\n",
    "\t\"dataset_hash\": dataset_hash,\n",
    "\t\"configuration\": DATASET_SPECS.model_dump()\n",
    "}\n",
    "\n",
    "for key, entry in RESULTS_BY_TYPE.items():\n",
    "\tyes_count = sum([1 for e in entry if \"YES\" in e])\n",
    "\tno_count = sum([1 for e in entry if not \"YES\" in e])\n",
    "\t\n",
    "\t# no_count = sum([1 for e in entry if \"NO\" in e])\n",
    "    \n",
    "\tRESULTS_PERCENTAGES[key] = {\n",
    "\t\t\"percentage\": 100 * yes_count / max(1, (yes_count + no_count)),\n",
    "\t\t\"samples\": (yes_count + no_count)\n",
    "\t}\n",
    "\n",
    "\n",
    "print(json.dumps(RESULTS_PERCENTAGES, indent=4))\n",
    "\n",
    "with open(f\"results_percentages_{str(int(time.time()))[:]}.json\", \"w\") as f:\n",
    "\tjson.dump(RESULTS_PERCENTAGES, f, indent=4)\n",
    "\tf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QL_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
